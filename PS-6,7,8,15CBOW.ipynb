{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 93\n",
      "Word Index: {'the': 1, 'of': 2, 'transmission': 3, 'influenza': 4, 'covid': 5, 'virus': 6, 'for': 7, 'is': 8, 'to': 9, 'a': 10, 'and': 11, 'between': 12, 'serial': 13, 'interval': 14, 'than': 15, 'be': 16, 'days': 17, 'are': 18, 'viruses': 19, 'shorter': 20, 'time': 21, 'from': 22, 'appearance': 23, 'symptoms': 24, 'while': 25, 'this': 26, 'that': 27, 'can': 28, 'in': 29, 'major': 30, 'driver': 31, 'number': 32, 'speed': 33, 'an': 34, 'important': 35, 'point': 36, 'difference': 37, 'two': 38, 'has': 39, 'median': 40, 'incubation': 41, 'period': 42, 'infection': 43, 'successive': 44, 'cases': 45, 'estimated': 46, 'means': 47, 'spread': 48, 'faster': 49, 'further': 50, 'first': 51, 'illness': 52, 'or': 53, 'potentially': 54, 'presymptomatic': 55, 'before': 56, 'contrast': 57, 'we': 58, 'learning': 59, 'there': 60, 'people': 61, 'who': 62, 'shed': 63, 'hours': 64, 'prior': 65, 'symptom': 66, 'onset': 67, 'at': 68, 'present': 69, 'does': 70, 'not': 71, 'appear': 72, 'reproductive': 73, 'secondary': 74, 'infections': 75, 'generated': 76, 'one': 77, 'infected': 78, 'individual': 79, 'understood': 80, 'higher': 81, 'however': 82, 'estimates': 83, 'both': 84, 'very': 85, 'context': 86, 'timespecific': 87, 'making': 88, 'direct': 89, 'comparisons': 90, 'more': 91, 'difficult': 92}\n",
      "Tokenized Sequences: [1, 33, 2, 3, 8, 34, 35, 36, 2, 37, 12, 1, 38, 19, 4, 39, 10, 20, 40, 41, 42, 1, 21, 22, 43, 9, 23, 2, 24, 11, 10, 20, 13, 14, 1, 21, 12, 44, 45, 15, 5, 6, 1, 13, 14, 7, 5, 6, 8, 46, 9, 16, 17, 25, 7, 4, 6, 1, 13, 14, 8, 17, 26, 47, 27, 4, 28, 48, 49, 15, 5, 50, 3, 29, 1, 51, 17, 2, 52, 53, 54, 55, 3, 3, 2, 1, 6, 56, 1, 23, 2, 24, 8, 10, 30, 31, 2, 3, 7, 4, 29, 57, 25, 58, 18, 59, 27, 60, 18, 61, 62, 28, 63, 5, 6, 64, 65, 9, 66, 67, 68, 69, 26, 70, 71, 72, 9, 16, 10, 30, 31, 2, 3, 1, 73, 32, 1, 32, 2, 74, 75, 76, 22, 77, 78, 79, 8, 80, 9, 16, 12, 11, 7, 5, 6, 81, 15, 7, 4, 82, 83, 7, 84, 5, 11, 4, 19, 18, 85, 86, 11, 87, 88, 89, 90, 91, 92]\n",
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.5344 - accuracy: 0.0231\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.5151 - accuracy: 0.0405\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4979 - accuracy: 0.0983\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4819 - accuracy: 0.1618\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 4.4643 - accuracy: 0.2948\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.4491 - accuracy: 0.3931\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4328 - accuracy: 0.4335\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.4141 - accuracy: 0.5087\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3940 - accuracy: 0.5838\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.3739 - accuracy: 0.6069\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3540 - accuracy: 0.6474\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3313 - accuracy: 0.6416\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.3070 - accuracy: 0.6821\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2858 - accuracy: 0.6936\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.2638 - accuracy: 0.7572\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2295 - accuracy: 0.7630\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.2086 - accuracy: 0.7341\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1751 - accuracy: 0.7688\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.1449 - accuracy: 0.7630\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 4.1116 - accuracy: 0.8035\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0776 - accuracy: 0.7514\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0365 - accuracy: 0.8092\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 4.0041 - accuracy: 0.7399\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9606 - accuracy: 0.7977\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.9145 - accuracy: 0.8150\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8762 - accuracy: 0.7977\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.8273 - accuracy: 0.7919\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7832 - accuracy: 0.7977\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.7318 - accuracy: 0.8208\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6735 - accuracy: 0.8092\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.6121 - accuracy: 0.8150\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.5532 - accuracy: 0.8324\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.4927 - accuracy: 0.8150\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.4375 - accuracy: 0.8497\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3718 - accuracy: 0.8324\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.3008 - accuracy: 0.8728\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 3.2418 - accuracy: 0.8613\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1707 - accuracy: 0.8671\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.1029 - accuracy: 0.8439\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 3.0400 - accuracy: 0.8439\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.9472 - accuracy: 0.8844\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.8638 - accuracy: 0.8497\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7791 - accuracy: 0.8728\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.7328 - accuracy: 0.8728\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.6446 - accuracy: 0.8382\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.5847 - accuracy: 0.8555\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.4962 - accuracy: 0.8728\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3859 - accuracy: 0.8728\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.3164 - accuracy: 0.8613\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.2688 - accuracy: 0.8902\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.1785 - accuracy: 0.8786\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0885 - accuracy: 0.9017\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 2.0281 - accuracy: 0.8844\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.9642 - accuracy: 0.8960\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8583 - accuracy: 0.8844\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.8240 - accuracy: 0.8902\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.7031 - accuracy: 0.9191\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6707 - accuracy: 0.9017\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.6061 - accuracy: 0.9075\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5509 - accuracy: 0.9075\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.5041 - accuracy: 0.9306\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.4230 - accuracy: 0.9306\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.3637 - accuracy: 0.9249\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2994 - accuracy: 0.9191\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.2464 - accuracy: 0.9480\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 1.1806 - accuracy: 0.9422\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 1.1400 - accuracy: 0.9133\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.1179 - accuracy: 0.9364\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 1.0300 - accuracy: 0.9364\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9992 - accuracy: 0.9422\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.9779 - accuracy: 0.9422\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.9219 - accuracy: 0.9538\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8601 - accuracy: 0.9480\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8381 - accuracy: 0.9595\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.8166 - accuracy: 0.9595\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7774 - accuracy: 0.9595\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.7177 - accuracy: 0.9653\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.7086 - accuracy: 0.9653\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.6579 - accuracy: 0.9769\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6471 - accuracy: 0.9711\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.6324 - accuracy: 0.9595\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5843 - accuracy: 0.9538\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5700 - accuracy: 0.9653\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5815 - accuracy: 0.9538\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.5326 - accuracy: 0.9769\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5158 - accuracy: 0.9711\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.5002 - accuracy: 0.9769\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4886 - accuracy: 0.9653\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4386 - accuracy: 0.9769\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4449 - accuracy: 0.9769\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4426 - accuracy: 0.9711\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4225 - accuracy: 0.9827\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.4025 - accuracy: 0.9769\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3866 - accuracy: 0.9769\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3886 - accuracy: 0.9769\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 7ms/step - loss: 0.3747 - accuracy: 0.9769\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3570 - accuracy: 0.9769\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3534 - accuracy: 0.9827\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 4ms/step - loss: 0.3442 - accuracy: 0.9827\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 5ms/step - loss: 0.3230 - accuracy: 0.9884\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Context: ['influenza', 'viruses']\n",
      "Predicted Target Word: the\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, Activation, Flatten, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import re\n",
    "\n",
    "# Step 1: Preprocess the Text\n",
    "document = \"\"\"The speed of transmission is an important point of difference between the two viruses. \n",
    "Influenza has a shorter median incubation period (the time from infection to appearance of symptoms) and a shorter serial interval (the time between successive cases) than COVID-19 virus. \n",
    "The serial interval for COVID-19 virus is estimated to be 5-6 days, while for influenza virus, the serial interval is 3 days. This means that influenza can spread faster than COVID-19. \n",
    "\n",
    "Further, transmission in the first 3-5 days of illness, or potentially pre-symptomatic transmission –transmission of the virus before the appearance of symptoms – is a major driver of transmission for influenza. \n",
    "In contrast, while we are learning that there are people who can shed COVID-19 virus 24-48 hours prior to symptom onset, at present, this does not appear to be a major driver of transmission. \n",
    "\n",
    "The reproductive number – the number of secondary infections generated from one infected individual – is understood to be between 2 and 2.5 for COVID-19 virus, higher than for influenza. However, estimates for both COVID-19 and influenza viruses are very context and time-specific, making direct comparisons more difficult.\"\"\"\n",
    "\n",
    "# Clean and tokenize the text\n",
    "document = document.lower()\n",
    "document = re.sub(r'[^a-z\\s]', '', document)  # Remove non-alphabetic characters\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([document])\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "index_word = tokenizer.index_word\n",
    "sequences = tokenizer.texts_to_sequences([document])[0]\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Word Index: {word_index}\")\n",
    "print(f\"Tokenized Sequences: {sequences}\")\n",
    "\n",
    "# Step 2: Generate Context-Target Pairs for CBOW\n",
    "window_size = 2  # Use context of 2 words before and after the target word\n",
    "context = []\n",
    "target = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(window_size, len(sequences) - window_size):\n",
    "    context.append([sequences[i - window_size], sequences[i + window_size]])\n",
    "    target.append(sequences[i])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "context = np.array(context)\n",
    "target = np.array(target)\n",
    "\n",
    "embedding_dim = 50  # Size of the word embedding\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=2))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the CBOW Model\n",
    "context = np.array([context[:, 0], context[:, 1]]).T  # Reshape for context words\n",
    "model.fit(context, target, epochs=100, verbose=1)\n",
    "\n",
    "# Step 5: Predicting the Target Word from Context\n",
    "def predict_word(context_words):\n",
    "    # Convert context words to their corresponding indices\n",
    "    context_indices = [word_index[word] for word in context_words]\n",
    "    \n",
    "    # Reshape context for the model (it expects a 2D array)\n",
    "    context_input = np.array(context_indices).reshape(1, -1)\n",
    "    \n",
    "    # Predict the target word index\n",
    "    predicted = model.predict(context_input)\n",
    "    \n",
    "    # Convert the predicted index to the corresponding word\n",
    "    predicted_word_index = np.argmax(predicted, axis=1)[0]\n",
    "    predicted_word = index_word[predicted_word_index]\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Example Prediction - Predict target word for context words [\"speed\", \"of\"]\n",
    "context_test = [\"influenza\", \"viruses\"]\n",
    "predicted_word = predict_word(context_test)\n",
    "\n",
    "print(f\"Context: {context_test}\")\n",
    "print(f\"Predicted Target Word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Context: ['covid', 'influenza']\n",
      "Predicted Target Word: transmission\n"
     ]
    }
   ],
   "source": [
    "context_test = [\"covid\",\"influenza\"]\n",
    "predicted_word = predict_word(context_test)\n",
    "\n",
    "print(f\"Context: {context_test}\")\n",
    "print(f\"Predicted Target Word: {predicted_word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar words to 'influenza': {'viruses': ['faster', 'the', 'we', 'individual', 'one', 'both']}\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.layers[0].get_weights()[0]\n",
    "distance_matrix = np.sqrt(((embeddings[:, np.newaxis] - embeddings[np.newaxis, :]) ** 2).sum(axis=2))\n",
    "\n",
    "# Step 5: Define function to find similar words using distance matrix\n",
    "def find_similar_words(search_term, top_n=6):\n",
    "    search_term_idx = word_index.get(search_term)\n",
    "    \n",
    "    if not search_term_idx:\n",
    "        return f\"'{search_term}' not found in the vocabulary.\"\n",
    "    \n",
    "    similar_indices = distance_matrix[search_term_idx - 1].argsort()[1:top_n + 1]\n",
    "    similar_words = {search_term: [index_word[idx + 1] for idx in similar_indices]}\n",
    "    \n",
    "    return similar_words\n",
    "\n",
    "# Example: Find similar words to 'influenza'\n",
    "similar_words = find_similar_words('viruses')\n",
    "print(f\"Similar words to 'influenza': {similar_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
